# FSDP Training Example for FLUX.2 with LoHA
# Launch with: torchrun --nproc_per_node=4 run.py config/examples/train_flux2_loha_fsdp.yaml
#
# This config demonstrates FSDP (FullyShardedDataParallel) training across GPUs.
# Model parameters are sharded across ranks, enabling training without gradient checkpointing.
# Memory is reduced by ~4x with 4 GPUs, allowing larger batch sizes or disabling grad checkpointing.

job: extension
config:
  name: "flux2_loha_fsdp_[name]"
  process:
    - type: 'sd_trainer'
      
      training_folder: "output/flux2_loha_fsdp"
      
      device: cuda
      
      model:
        name_or_path: "black-forest-labs/FLUX.1-dev"
        is_flux: true
        quantize: false
        
      network:
        type: "loha"
        linear: 64
        linear_alpha: 32
        use_scalar: true
        weight_decompose: true
      
      train:
        # FSDP distributed training settings
        distributed: true
        dist_backend: "nccl"
        dist_mode: "fsdp"  # Use FSDP instead of DDP
        
        # FSDP-specific options
        fsdp_mixed_precision: "bf16"
        fsdp_cpu_offload: false  # Enable to further reduce GPU memory
        
        # Training params - can disable gradient checkpointing with FSDP!
        batch_size: 1
        steps: 1000
        lr: 1e-4
        optimizer: "adamw"
        lr_scheduler: "cosine"
        
        dtype: "bf16"
        gradient_checkpointing: false  # Not needed with FSDP sharding!
        max_grad_norm: 1.0
        
      datasets:
        - folder_path: "/path/to/your/dataset"
          caption_ext: "txt"
          resolution: 1024
          buckets: true
          
      save:
        dtype: "bf16"
        save_every: 250
        max_step_saves_to_keep: 3
        
      sample:
        sampler: "flowmatch"
        sample_every: 250
        width: 1024
        height: 1024
        prompts:
          - "a photo of [trigger] in a forest"
          - "a portrait of [trigger] smiling"
        seed: 42
        
meta:
  name: "[name]"
  version: "1.0"
