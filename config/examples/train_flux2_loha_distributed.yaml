# Distributed Training Example for FLUX.2 with LoHA
# Launch with: torchrun --nproc_per_node=4 run.py config/examples/train_flux2_loha_distributed.yaml
#
# This config demonstrates DDP (DistributedDataParallel) training across 4 GPUs.
# Each GPU gets a replica of the full model; gradients are synchronized across ranks.
# Effective batch size = batch_size * num_gpus = 1 * 4 = 4

job: extension
config:
  name: "flux2_loha_distributed_[name]"
  process:
    - type: 'sd_trainer'
      
      training_folder: "output/flux2_loha_distributed"
      
      device: cuda
      
      model:
        name_or_path: "black-forest-labs/FLUX.1-dev"
        is_flux: true
        quantize: false
        
      network:
        type: "loha"
        linear: 64
        linear_alpha: 32
        use_scalar: true
        weight_decompose: true
      
      train:
        # Distributed training settings
        distributed: true
        dist_backend: "nccl"
        dist_mode: "ddp"  # Use pure PyTorch DDP
        
        # Training params
        batch_size: 1  # Per-GPU batch size. Effective = 1 * 4 GPUs = 4
        steps: 1000
        lr: 1e-4
        optimizer: "adamw"
        lr_scheduler: "cosine"
        
        dtype: "bf16"
        gradient_checkpointing: true
        max_grad_norm: 1.0
        
      datasets:
        - folder_path: "/path/to/your/dataset"
          caption_ext: "txt"
          resolution: 1024
          buckets: true
          
      save:
        dtype: "bf16"
        save_every: 250
        max_step_saves_to_keep: 3
        
      sample:
        sampler: "flowmatch"
        sample_every: 250
        width: 1024
        height: 1024
        prompts:
          - "a photo of [trigger] in a forest"
          - "a portrait of [trigger] smiling"
        seed: 42
        
meta:
  name: "[name]"
  version: "1.0"
