---
# FLUX.2 LoRA Training Configuration - 4 GPU Deterministic Split
#
# This configuration demonstrates deterministic GPU splitting for FLUX.2 training
# across 4 GPUs. The deterministic split ensures consistent memory distribution
# and predictable behavior across runs.
#
# FLUX.2 Architecture:
#   - 8 double blocks (DoubleStreamBlock): Process image and text streams separately
#   - 48 single blocks (SingleStreamBlock): Process concatenated img+txt stream
#
# Default 4-GPU Split (even distribution):
#   - GPU 0: 2 double blocks + 12 single blocks + embedders/output layers
#   - GPU 1: 2 double blocks + 12 single blocks
#   - GPU 2: 2 double blocks + 12 single blocks
#   - GPU 3: 2 double blocks + 12 single blocks
#
# Memory estimate per GPU with BF16: ~25GB transformer, text encoder on GPU 0

job: extension
config:
  name: "flux2_lora_4gpu_deterministic"
  process:
    - type: 'sd_trainer'
      training_folder: "output"
      device: cuda:0  # Primary device, splitter distributes to all GPUs

      network:
        type: "lora"
        linear: 32  # Higher rank possible with more VRAM per GPU
        linear_alpha: 32

      save:
        dtype: float16
        save_every: 500
        max_step_saves_to_keep: 3

      datasets:
        - folder_path: "/path/to/images/folder"
          caption_ext: "txt"
          caption_dropout_rate: 0.05
          shuffle_tokens: false
          cache_latents_to_disk: true
          resolution: [512, 768, 1024]

      train:
        batch_size: 2  # Can increase batch size with 4 GPUs
        steps: 3000
        gradient_accumulation_steps: 1
        train_unet: true
        train_text_encoder: false
        gradient_checkpointing: false  # Disabled - GPU split provides memory relief
        noise_scheduler: "flowmatch"
        optimizer: "adamw8bit"
        lr: 1e-4
        dtype: bf16

        ema_config:
          use_ema: true
          ema_decay: 0.99

      model:
        name_or_path: "black-forest-labs/FLUX.2-dev"
        arch: "flux2"
        quantize: true

        # Enable GPU splitting across all available GPUs
        split_model_over_gpus: true

        # Deterministic split uses defaults for 4 GPUs:
        # gpu_split_double: [2, 2, 2, 2]      # 2 double blocks per GPU (sum=8)
        # gpu_split_single: [12, 12, 12, 12]  # 12 single blocks per GPU (sum=48)

      sample:
        sampler: "flowmatch"
        sample_every: 500
        width: 1024
        height: 1024
        prompts:
          - "a professional photo of a woman with red hair"
          - "a landscape photograph of mountains at sunset"
          - "a detailed portrait of a man in a business suit"
          - "an artistic rendering of a futuristic city"
        neg: ""
        seed: 42
        walk_seed: true
        guidance_scale: 4
        sample_steps: 20

meta:
  name: "[name]"
  version: '1.0'
  description: "FLUX.2 LoRA training with deterministic 4-GPU split"
  gpu_config:
    num_gpus: 4
    split_type: "deterministic_default"
    double_blocks_per_gpu: [2, 2, 2, 2]
    single_blocks_per_gpu: [12, 12, 12, 12]
